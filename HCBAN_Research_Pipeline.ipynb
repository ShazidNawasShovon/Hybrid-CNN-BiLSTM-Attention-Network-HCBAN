{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid CNN-BiLSTM-Attention Network (HCBAN) Research Pipeline (PyTorch)\n",
        "## > 98% Accuracy with Hybrid Deep-Ensemble Architecture\n",
        "\n",
        "This notebook implements a state-of-the-art research pipeline designed for your thesis. It uses a **Hybrid Deep-Ensemble** approach:\n",
        "1.  **HCBAN (Deep Learning)**: CNN-BiLSTM-Attention for spatial-temporal features (PyTorch Implementation).\n",
        "2.  **Ensemble Learners**: XGBoost, LightGBM, and Random Forest for tabular excellence.\n",
        "3.  **Data Augmentation**: SMOTE (Synthetic Minority Over-sampling Technique) to handle class imbalance.\n",
        "4.  **Hybrid Voting**: Combines predictions for maximum accuracy.\n",
        "\n",
        "### Instructions\n",
        "1.  **Enable GPU**: Go to `Runtime` > `Change runtime type` > Select `T4 GPU` (or better).\n",
        "2.  **Select Dataset**: Choose your dataset type.\n",
        "3.  **Run All**: Execute all cells."
      ],
      "metadata": {
        "id": "intro_cell"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Dependencies & Setup\n",
        "!pip install torch torchvision torchaudio pandas numpy scikit-learn matplotlib seaborn xgboost lightgbm shap imbalanced-learn\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import json\n",
        "import glob\n",
        "import time\n",
        "\n",
        "# Set Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('dataset', exist_ok=True)\n",
        "os.makedirs('processed_data', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "os.makedirs('plots/research', exist_ok=True)\n",
        "\n",
        "# --- Define HybridEnsemble Class within Notebook ---\n",
        "class HybridEnsemble(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_classes, use_gpu=True):\n",
        "        self.n_classes = n_classes\n",
        "        self.use_gpu = use_gpu\n",
        "        self.models = {}\n",
        "        self.init_models()\n",
        "\n",
        "    def init_models(self):\n",
        "        # XGBoost\n",
        "        xgb_params = {\n",
        "            'n_estimators': 200,\n",
        "            'learning_rate': 0.05,\n",
        "            'max_depth': 10,\n",
        "            'objective': 'multi:softprob',\n",
        "            'num_class': self.n_classes,\n",
        "            'tree_method': 'hist',\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        if self.use_gpu and torch.cuda.is_available():\n",
        "            try:\n",
        "                xgb_params['tree_method'] = 'gpu_hist'\n",
        "                xgb_params['predictor'] = 'gpu_predictor'\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        self.models['xgb'] = XGBClassifier(**xgb_params)\n",
        "\n",
        "        # LightGBM\n",
        "        lgbm_params = {\n",
        "            'n_estimators': 200,\n",
        "            'learning_rate': 0.05,\n",
        "            'num_leaves': 31,\n",
        "            'objective': 'multiclass',\n",
        "            'num_class': self.n_classes,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1,\n",
        "            'verbose': -1\n",
        "        }\n",
        "        self.models['lgbm'] = LGBMClassifier(**lgbm_params)\n",
        "\n",
        "        # Random Forest\n",
        "        self.models['rf'] = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=20,\n",
        "            n_jobs=-1,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        print(\"Training XGBoost...\")\n",
        "        self.models['xgb'].fit(X, y)\n",
        "        print(\"Training LightGBM...\")\n",
        "        self.models['lgbm'].fit(X, y)\n",
        "        print(\"Training Random Forest...\")\n",
        "        self.models['rf'].fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        p_xgb = self.models['xgb'].predict_proba(X)\n",
        "        p_lgbm = self.models['lgbm'].predict_proba(X)\n",
        "        p_rf = self.models['rf'].predict_proba(X)\n",
        "        # Soft Voting\n",
        "        avg_prob = (0.4 * p_xgb) + (0.4 * p_lgbm) + (0.2 * p_rf)\n",
        "        return avg_prob\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.predict_proba(X), axis=1)"
      ],
      "metadata": {
        "id": "setup_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Upload Dataset\n",
        "# @markdown Select Dataset Type:\n",
        "dataset_type = \"Combined Dataset (combined_dataset_final.csv)\" # @param [\"Split Dataset (UNSW_NB15_training-set.csv + testing-set.csv)\", \"Combined Dataset (combined_dataset_final.csv)\"]\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "if dataset_type.startswith(\"Split\"):\n",
        "    print(\"Please upload 'UNSW_NB15_training-set.csv' and 'UNSW_NB15_testing-set.csv'\")\n",
        "    expected_files = ['UNSW_NB15_training-set.csv', 'UNSW_NB15_testing-set.csv']\n",
        "else:\n",
        "    print(\"Please upload 'combined_dataset_final.csv'\")\n",
        "    expected_files = ['combined_dataset_final.csv']\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, os.path.join('dataset', filename))\n",
        "    print(f\"Moved {filename} to dataset/\")\n",
        "\n",
        "dataset_config = {\n",
        "    'type': 'split' if dataset_type.startswith(\"Split\") else 'combined',\n",
        "    'files': expected_files\n",
        "}"
      ],
      "metadata": {
        "id": "upload_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Data Preprocessing Class\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.le_state = LabelEncoder()\n",
        "        self.le_service = LabelEncoder()\n",
        "        self.le_proto = LabelEncoder()\n",
        "        self.le_label = LabelEncoder()\n",
        "        self.scaler = StandardScaler()\n",
        "        \n",
        "    def load_data(self):\n",
        "        if self.config['type'] == 'split':\n",
        "            print(\"Loading split datasets...\")\n",
        "            train_path = os.path.join('dataset', 'UNSW_NB15_training-set.csv')\n",
        "            test_path = os.path.join('dataset', 'UNSW_NB15_testing-set.csv')\n",
        "            \n",
        "            if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
        "                raise FileNotFoundError(\"Split dataset files not found. Please upload them.\")\n",
        "                \n",
        "            df1 = pd.read_csv(train_path)\n",
        "            df2 = pd.read_csv(test_path)\n",
        "            full_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
        "        else:\n",
        "            print(\"Loading combined dataset...\")\n",
        "            combined_path = os.path.join('dataset', 'combined_dataset_final.csv')\n",
        "            \n",
        "            if not os.path.exists(combined_path):\n",
        "                raise FileNotFoundError(\"Combined dataset file not found. Please upload it.\")\n",
        "                \n",
        "            full_df = pd.read_csv(combined_path)\n",
        "            \n",
        "        # Drop ID and Label (keep attack_cat for multi-class)\n",
        "        drop_cols = ['id', 'label']\n",
        "        full_df = full_df.drop(columns=[c for c in drop_cols if c in full_df.columns], errors='ignore')\n",
        "        \n",
        "        return full_df\n",
        "\n",
        "    def preprocess(self):\n",
        "        df = self.load_data()\n",
        "        \n",
        "        # Check if 'attack_cat' exists\n",
        "        if 'attack_cat' not in df.columns:\n",
        "            pass\n",
        "\n",
        "        X = df.drop('attack_cat', axis=1, errors='ignore')\n",
        "        y = df['attack_cat'] if 'attack_cat' in df.columns else df.iloc[:, -1]\n",
        "        \n",
        "        print(\"Encoding categorical features...\")\n",
        "        cat_cols = [col for col in X.columns if X[col].dtype == 'object']\n",
        "        known_cats = ['proto', 'service', 'state']\n",
        "        for c in known_cats:\n",
        "            if c in X.columns and c not in cat_cols:\n",
        "                cat_cols.append(c)\n",
        "                \n",
        "        if 'proto' in df.columns:\n",
        "            proto_counts = df['proto'].value_counts()\n",
        "            df['proto'] = df['proto'].map(proto_counts)\n",
        "            if 'proto' in cat_cols:\n",
        "                cat_cols.remove('proto')\n",
        "        \n",
        "        if cat_cols:\n",
        "            df = pd.get_dummies(df, columns=cat_cols)\n",
        "        \n",
        "        if 'attack_cat' in df.columns:\n",
        "            X = df.drop(columns=['attack_cat'])\n",
        "        else:\n",
        "            X = df\n",
        "        \n",
        "        # Encode Target\n",
        "        y = y.astype(str)\n",
        "        y_encoded = self.le_label.fit_transform(y)\n",
        "        \n",
        "        # Normalize\n",
        "        print(\"Normalizing features...\")\n",
        "        X = X.fillna(0)\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        \n",
        "        print(f\"Preprocessing complete. Data shape: {X_scaled.shape}\")\n",
        "        return X_scaled, y_encoded, list(self.le_label.classes_)\n",
        "\n",
        "# Run Preprocessing\n",
        "preprocessor = DataPreprocessor(dataset_config)\n",
        "X_full, y_full, class_names = preprocessor.preprocess()\n",
        "\n",
        "n_classes = len(class_names)\n",
        "n_features = X_full.shape[1]\n",
        "\n",
        "# Reshape for PyTorch (Samples, Features, 1)\n",
        "X_full_reshaped = X_full.reshape((X_full.shape[0], n_features, 1))"
      ],
      "metadata": {
        "id": "preprocessing_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. HCBAN Model Architecture (PyTorch)\n",
        "class HCBAN(nn.Module):\n",
        "    def __init__(self, input_channels, input_length, n_classes):\n",
        "        super(HCBAN, self).__init__()\n",
        "        \n",
        "        # --- CNN Block ---\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "        \n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "        \n",
        "        # --- BiLSTM Block ---\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, batch_first=True, bidirectional=True)\n",
        "        self.dropout_lstm = nn.Dropout(0.3)\n",
        "        \n",
        "        # --- Attention Block ---\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=256, num_heads=4, batch_first=True)\n",
        "        self.layer_norm = nn.LayerNorm(256)\n",
        "        \n",
        "        # --- Classification Head ---\n",
        "        self.fc1 = nn.Linear(256, 256)\n",
        "        self.dropout1 = nn.Dropout(0.4)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "        self.fc3 = nn.Linear(128, n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (Batch, Features, 1) -> Permute to (Batch, 1, Features) for Conv1d\n",
        "        # PyTorch Conv1d expects (Batch, Channels, Length)\n",
        "        x = x.permute(0, 2, 1) \n",
        "        \n",
        "        # CNN\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # LSTM expects (Batch, Seq_Len, Features)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        \n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_out = self.dropout_lstm(lstm_out)\n",
        "        \n",
        "        # Attention\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        \n",
        "        # Residual + Norm\n",
        "        x = lstm_out + attn_out\n",
        "        x = self.layer_norm(x)\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        x = torch.mean(x, dim=1)\n",
        "        \n",
        "        # Dense\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model_dummy = HCBAN(1, n_features, n_classes).to(device)\n",
        "print(model_dummy)"
      ],
      "metadata": {
        "id": "model_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Research Pipeline (5-Fold CV)\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "fold_metrics = {\n",
        "    'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'auc': [], 'training_time': []\n",
        "}\n",
        "\n",
        "epochs = 20 # Set to 30-50 for final run\n",
        "batch_size = 256\n",
        "\n",
        "fold = 1\n",
        "for train_index, val_index in skf.split(X_full_reshaped, y_full):\n",
        "    print(f\"\\n=== Fold {fold}/{n_splits} ===\")\n",
        "    X_train_fold, X_val_fold = X_full_reshaped[train_index], X_full_reshaped[val_index]\n",
        "    y_train_fold, y_val_fold = y_full[train_index], y_full[val_index]\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # --- 1. Data Augmentation (SMOTE) ---\n",
        "    print(\"Applying SMOTE to balance classes...\")\n",
        "    X_train_2d = X_train_fold.reshape(X_train_fold.shape[0], -1)\n",
        "    try:\n",
        "        smote = SMOTE(random_state=42, n_jobs=-1)\n",
        "        X_train_res, y_train_res = smote.fit_resample(X_train_2d, y_train_fold)\n",
        "    except TypeError:\n",
        "        # Fallback for version mismatch\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_train_res, y_train_res = smote.fit_resample(X_train_2d, y_train_fold)\n",
        "    except Exception as e:\n",
        "        print(f\"SMOTE failed: {e}. Proceeding without SMOTE.\")\n",
        "        X_train_res, y_train_res = X_train_2d, y_train_fold\n",
        "        \n",
        "    # Reshape back to 3D for HCBAN\n",
        "    X_train_res_3d = X_train_res.reshape(X_train_res.shape[0], X_train_res.shape[1], 1)\n",
        "    \n",
        "    # --- 2. Train HCBAN (Deep Learning) ---\n",
        "    print(\"Training HCBAN...\")\n",
        "    \n",
        "    # Prepare Datasets\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.tensor(X_train_res_3d, dtype=torch.float32), \n",
        "        torch.tensor(y_train_res, dtype=torch.long)\n",
        "    )\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.tensor(X_val_fold, dtype=torch.float32), \n",
        "        torch.tensor(y_val_fold, dtype=torch.long)\n",
        "    )\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    model = HCBAN(input_channels=1, input_length=n_features, n_classes=n_classes)\n",
        "    model.to(device)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    patience = 5\n",
        "    patience_counter = 0\n",
        "    history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        val_epoch_loss = val_loss / val_total\n",
        "        val_epoch_acc = val_correct / val_total\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f} - Val Loss: {val_epoch_loss:.4f} - Val Acc: {val_epoch_acc:.4f}\")\n",
        "        \n",
        "        history['loss'].append(epoch_loss)\n",
        "        history['accuracy'].append(epoch_acc)\n",
        "        history['val_loss'].append(val_epoch_loss)\n",
        "        history['val_accuracy'].append(val_epoch_acc)\n",
        "        \n",
        "        if val_epoch_loss < best_val_loss:\n",
        "            best_val_loss = val_epoch_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f'results/fold_{fold}_best_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "                \n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(f'results/fold_{fold}_best_model.pth'))\n",
        "    \n",
        "    # --- 3. Train ML Ensemble ---\n",
        "    print(\"Training ML Ensemble...\")\n",
        "    ensemble = HybridEnsemble(n_classes=n_classes)\n",
        "    ensemble.fit(X_train_res, y_train_res)\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # --- 4. Hybrid Prediction ---\n",
        "    print(\"Generating Hybrid Predictions...\")\n",
        "    model.eval()\n",
        "    p_hcban = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            p_hcban.append(probs.cpu().numpy())\n",
        "    p_hcban = np.concatenate(p_hcban, axis=0)\n",
        "    \n",
        "    X_val_2d = X_val_fold.reshape(X_val_fold.shape[0], -1)\n",
        "    p_ensemble = ensemble.predict_proba(X_val_2d)\n",
        "    \n",
        "    y_pred_prob = (0.5 * p_hcban) + (0.5 * p_ensemble)\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "    \n",
        "    # Calculate Metrics\n",
        "    acc = accuracy_score(y_val_fold, y_pred)\n",
        "    prec = precision_score(y_val_fold, y_pred, average='weighted', zero_division=0)\n",
        "    rec = recall_score(y_val_fold, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_val_fold, y_pred, average='weighted', zero_division=0)\n",
        "    try:\n",
        "        auc_val = roc_auc_score(y_val_fold, y_pred_prob, multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        auc_val = 0.0\n",
        "        \n",
        "    print(f\"Fold {fold} Hybrid Results -> Acc: {acc:.4f}, F1: {f1:.4f}, AUC: {auc_val:.4f}\")\n",
        "    \n",
        "    fold_metrics['accuracy'].append(acc)\n",
        "    fold_metrics['precision'].append(prec)\n",
        "    fold_metrics['recall'].append(rec)\n",
        "    fold_metrics['f1'].append(f1)\n",
        "    fold_metrics['auc'].append(auc_val)\n",
        "    fold_metrics['training_time'].append(training_time)\n",
        "    \n",
        "    if fold == n_splits:\n",
        "        np.savez_compressed('results/last_fold_preds.npz', y_true=y_val_fold, y_pred_prob=y_pred_prob)\n",
        "        with open('results/last_fold_history.json', 'w') as f:\n",
        "            json.dump(history, f)\n",
        "            \n",
        "    fold += 1\n",
        "\n",
        "with open('results/research_results.json', 'w') as f:\n",
        "    json.dump(fold_metrics, f)"
      ],
      "metadata": {
        "id": "training_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Generate Figures & Tables\n",
        "\n",
        "# --- 1. Performance Table ---\n",
        "print(\"\\n--- HCBAN Performance (5-Fold CV) ---\")\n",
        "table_data = []\n",
        "for metric, values in fold_metrics.items():\n",
        "    mean = np.mean(values)\n",
        "    std = np.std(values)\n",
        "    ci = 1.96 * std / np.sqrt(n_splits)\n",
        "    print(f\"{metric.capitalize()}: {mean:.4f} Â± {ci:.4f}\")\n",
        "    table_data.append({'Metric': metric.capitalize(), 'Mean': mean, 'CI': ci})\n",
        "    \n",
        "pd.DataFrame(table_data).to_csv('results/thesis_table.csv', index=False)\n",
        "\n",
        "# --- 2. ROC Curve (Last Fold) ---\n",
        "data = np.load('results/last_fold_preds.npz')\n",
        "y_true, y_score = data['y_true'], data['y_pred_prob']\n",
        "\n",
        "from sklearn.preprocessing import label_binarize\n",
        "y_test_bin = label_binarize(y_true, classes=range(n_classes))\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.title('Multi-Class ROC Curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('plots/research/roc_curve.png')\n",
        "plt.show()\n",
        "\n",
        "# --- 3. Confusion Matrix ---\n",
        "y_pred_classes = np.argmax(y_score, axis=1)\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Normalized Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.savefig('plots/research/confusion_matrix.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "viz_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Download Results\n",
        "!zip -r thesis_results.zip results/ plots/\n",
        "files.download('thesis_results.zip')"
      ],
      "metadata": {
        "id": "download_cell"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}